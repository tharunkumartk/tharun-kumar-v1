---
company: "Google DeepMind"
position: "Research Engineer"
startDate: "2025-09-01"
endDate: "present"
companyUrl: "https://sites.google.com/view/gbrainprinceton/projects/spectral-transformers"
skills: ["Python", "PyTorch", "State Space Models", "LLMs", "Machine Learning", "Research", "Spectral Filtering"]
order: -1
---

Researching and implementing novel State Space Model architectures using Spectral Filtering for efficient long-context language modeling under Prof. Elad Hazan in the Princeton Google DeepMind Lab. Training 500M, 1B, and 7B parameter LLMs from scratch, benchmarking performance against transformer and SSM baselines, achieving state of the art performance for language modeling in O(n) time (instead of transformers' O(n^2) time). 

This research focuses on making language models more efficient at processing very long sequences of text, which is crucial for applications like document analysis and extended conversations.
